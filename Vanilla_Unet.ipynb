{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "from data import *\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.backend import int_shape\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint \n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.layers import (BatchNormalization, Conv2D, Conv2DTranspose, MaxPooling2D, Dropout, SpatialDropout2D, \n",
    "                                     UpSampling2D, Input, concatenate, multiply, add, Activation, Cropping2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 images belonging to 1 classes.\n",
      "Found 2 images belonging to 1 classes.\n",
      "Found 12 images belonging to 1 classes.\n",
      "Found 2 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Tried data from the same distribution, split data from our satelite images into train and validation sets\n",
    "\n",
    "def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict=data_gen_args,image_color_mode = \"rgb\",\n",
    "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
    "                    flag_multi_class = False,num_class = 2,save_to_dir = None,\n",
    "                   target_size = (256,256),seed = 1):\n",
    "    '''\n",
    "    can generate image and mask at the same time\n",
    "    use the same seed for image_datagen and mask_datagen to ensure the transformation for image and mask is the same\n",
    "    if you want to visualize the results of generator, set save_to_dir = \"your path\"\n",
    "    '''\n",
    "    image_datagen = ImageDataGenerator(**aug_dict, validation_split=0.2, rescale=1./255)\n",
    "    mask_datagen = ImageDataGenerator(**aug_dict, validation_split=0.2, rescale=1./255)\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        subset = 'training',\n",
    "        seed = seed) \n",
    "    image_val_generator = image_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [image_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = image_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = image_save_prefix,\n",
    "        subset = 'validation',\n",
    "        seed = seed)\n",
    "    \n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        subset = 'training',\n",
    "        seed = seed)\n",
    "    mask_val_generator = mask_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        classes = [mask_folder],\n",
    "        class_mode = None,\n",
    "        color_mode = mask_color_mode,\n",
    "        target_size = target_size,\n",
    "        batch_size = batch_size,\n",
    "        save_to_dir = save_to_dir,\n",
    "        save_prefix  = mask_save_prefix,\n",
    "        subset = 'validation',\n",
    "        seed = seed)\n",
    "    \n",
    "        \n",
    "    return image_generator, image_val_generator, mask_generator, mask_val_generator\n",
    "\n",
    "image_generator, image_val_generator, mask_generator, mask_val_generator = trainGenerator(3,'dataset','images_satelite','labels_satelite',data_gen_args)\n",
    "\n",
    "def train(image_generator, mask_generator, flag_multi_class = False, num_class = 2):\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "    for (img,mask) in train_generator:\n",
    "        img,mask = adjustData(img,mask,flag_multi_class,num_class)       \n",
    "        yield (img,mask)\n",
    "        \n",
    "def val(image_val_generator, mask_val_generator, flag_multi_class = False, num_class = 2):\n",
    "    val_generator = zip(image_val_generator, mask_val_generator)\n",
    "    for (img_val,mask_val) in val_generator:\n",
    "        img_val,mask_val = adjustData(img_val,mask_val,flag_multi_class,num_class)       \n",
    "        yield (img_val,mask_val)\n",
    "        \n",
    "train_generator = train(image_generator, mask_generator)\n",
    "val_generator = val(image_val_generator, mask_val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train form data in dataset using data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilla unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_crop_shape( target, refer):\n",
    "\n",
    "    # width, the 3rd dimension\n",
    "    cw = (target.get_shape()[2] - refer.get_shape()[2])\n",
    "\n",
    "    assert (cw >= 0)\n",
    "\n",
    "    if cw % 2 != 0:\n",
    "\n",
    "        cw1, cw2 = int(cw/2), int(cw/2) + 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        cw1, cw2 = int(cw/2), int(cw/2)\n",
    "\n",
    "    # height, the 2nd dimension\n",
    "\n",
    "    ch = (target.get_shape()[1] - refer.get_shape()[1])\n",
    "\n",
    "    assert (ch >= 0)\n",
    "\n",
    "    if ch % 2 != 0:\n",
    "\n",
    "        ch1, ch2 = int(ch/2), int(ch/2) + 1\n",
    "\n",
    "    else:\n",
    "\n",
    "        ch1, ch2 = int(ch/2), int(ch/2)\n",
    "\n",
    "\n",
    "\n",
    "    return (ch1, ch2), (cw1, cw2)\n",
    "\n",
    "def VanillaUnet( num_class = 1, img_shape = (256,256,3)):\n",
    "\n",
    "    concat_axis = 3\n",
    "\n",
    "    # input\n",
    "    \n",
    "    inputs = layers.Input(shape = img_shape)\n",
    "\n",
    "\n",
    "    # Unet convolution block 1\n",
    "    \n",
    "    conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same', name='conv1_1')(inputs)\n",
    "\n",
    "    conv1 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "\n",
    "    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    \n",
    "\n",
    "    # Unet convolution block 2\n",
    "\n",
    "    conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "\n",
    "    conv2 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "\n",
    "    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "\n",
    "\n",
    "    # Unet convolution block 3\n",
    "\n",
    "    conv3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "\n",
    "    conv3 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "\n",
    "    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "\n",
    "\n",
    "    # Unet convolution block 4\n",
    "\n",
    "    conv4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "\n",
    "    conv4 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    print(conv4.shape)\n",
    "\n",
    "    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "\n",
    "\n",
    "    # Unet convolution block 5\n",
    "\n",
    "    conv5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n",
    "\n",
    "    conv5 = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "\n",
    "\n",
    "    # Unet up-sampling block 1; Concatenation with crop_conv4\n",
    "\n",
    "    up_conv5 = layers.UpSampling2D(size=(2, 2))(conv5)\n",
    "    print(up_conv5.shape)\n",
    "\n",
    "    ch, cw = get_crop_shape(conv4, up_conv5)\n",
    "\n",
    "    crop_conv4 = layers.Cropping2D(cropping=(ch,cw))(conv4)\n",
    "\n",
    "    up6 = layers.concatenate([up_conv5, crop_conv4], axis=concat_axis)\n",
    "\n",
    "    conv6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "\n",
    "    conv6 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "\n",
    "\n",
    "\n",
    "    # Unet up-sampling block 2; Concatenation with crop_conv3\n",
    "\n",
    "    up_conv6 = layers.UpSampling2D(size=(2, 2))(conv6)\n",
    "\n",
    "    ch, cw = get_crop_shape(conv3, up_conv6)\n",
    "\n",
    "    crop_conv3 = layers.Cropping2D(cropping=(ch,cw))(conv3)\n",
    "\n",
    "    up7 = layers.concatenate([up_conv6, crop_conv3], axis=concat_axis) \n",
    "\n",
    "    conv7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "\n",
    "    conv7 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "\n",
    "\n",
    "\n",
    "    # Unet up-sampling block 3; Concatenation with crop_conv2\n",
    "\n",
    "    up_conv7 = layers.UpSampling2D(size=(2, 2))(conv7)\n",
    "\n",
    "    ch, cw = get_crop_shape(conv2, up_conv7)\n",
    "\n",
    "    crop_conv2 = layers.Cropping2D(cropping=(ch,cw))(conv2)\n",
    "\n",
    "    up8 = layers.concatenate([up_conv7, crop_conv2], axis=concat_axis)\n",
    "\n",
    "    conv8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "\n",
    "    conv8 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "\n",
    "\n",
    "\n",
    "    # Unet up-sampling block 4; Concatenation with crop_conv1\n",
    "\n",
    "    up_conv8 = layers.UpSampling2D(size=(2, 2))(conv8)\n",
    "\n",
    "    ch, cw = get_crop_shape(conv1, up_conv8)\n",
    "\n",
    "    crop_conv1 = layers.Cropping2D(cropping=(ch,cw))(conv1)\n",
    "\n",
    "    up9 = layers.concatenate([up_conv8, crop_conv1], axis=concat_axis)\n",
    "\n",
    "    conv9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "\n",
    "    conv9 = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "\n",
    "\n",
    "\n",
    "    ch, cw = get_crop_shape(inputs, conv9)\n",
    "\n",
    "    conv9 = layers.ZeroPadding2D(padding=((ch[0], ch[1]), (cw[0], cw[1])))(conv9)\n",
    "\n",
    "    conv10 = layers.Conv2D(num_class, (1, 1))(conv9)\n",
    "    print(conv10.shape)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=conv10)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 32, 32, 256)\n",
      "(None, 32, 32, 512)\n",
      "(None, 256, 256, 1)\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 800 steps, validate for 120 steps\n",
      "Epoch 1/5\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.4127 - accuracy: 0.7921\n",
      "Epoch 00001: saving model to vanilla_unet_split.hdf5\n",
      "800/800 [==============================] - 80s 101ms/step - loss: 0.4126 - accuracy: 0.7921 - val_loss: 0.4574 - val_accuracy: 0.7260\n",
      "Epoch 2/5\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.4326 - accuracy: 0.7801\n",
      "Epoch 00002: saving model to vanilla_unet_split.hdf5\n",
      "800/800 [==============================] - 79s 99ms/step - loss: 0.4326 - accuracy: 0.7801 - val_loss: 0.4291 - val_accuracy: 0.8242\n",
      "Epoch 3/5\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.5351 - accuracy: 0.7342\n",
      "Epoch 00003: saving model to vanilla_unet_split.hdf5\n",
      "800/800 [==============================] - 80s 100ms/step - loss: 0.5352 - accuracy: 0.7341 - val_loss: 0.4306 - val_accuracy: 0.8218\n",
      "Epoch 4/5\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.5332 - accuracy: 0.7341\n",
      "Epoch 00004: saving model to vanilla_unet_split.hdf5\n",
      "800/800 [==============================] - 80s 100ms/step - loss: 0.5333 - accuracy: 0.7340 - val_loss: 0.4299 - val_accuracy: 0.8223\n",
      "Epoch 5/5\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.5328 - accuracy: 0.7344\n",
      "Epoch 00005: saving model to vanilla_unet_split.hdf5\n",
      "800/800 [==============================] - 80s 100ms/step - loss: 0.5328 - accuracy: 0.7344 - val_loss: 0.4301 - val_accuracy: 0.8229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff4d2afda10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen_args = dict(rotation_range=0.2,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "\n",
    "model = VanillaUnet()\n",
    "model_checkpoint = ModelCheckpoint('vanilla_unet_split.hdf5', monitor='val_accuracy',verbose=1, save_best_only=False)\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit_generator(train_generator,steps_per_epoch=800,epochs=5,validation_data=val_generator,validation_steps=120, callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=satellite_unet(pretrained='sat_unet.hdf5')\n",
    "#testGene = testGenerator(\"dataset/unlabelled\")\n",
    "#results = model.predict_generator(testGene,30,verbose=1)\n",
    "X_test,names=test(\"dataset/unlabelled\")\n",
    "preds=model.predict(X_test)\n",
    "preds=preds>0.5\n",
    "results=predToImgs(preds)\n",
    "saveResults(\"dataset/preds\",results,names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 256, 256, 3)\n",
      "(14, 256, 256, 1)\n"
     ]
    }
   ],
   "source": [
    "def test(test_path,target_size=(256,256), color_mode = \"rgb\"):\n",
    "    X_test=[]\n",
    "    names=[]\n",
    "    for filename in os.listdir(test_path):\n",
    "        name, ext = os.path.splitext(filename)\n",
    "        if ext!=\".png\" and ext!=\".jpg\":\n",
    "            continue\n",
    "        names.append(filename)\n",
    "        img=load_img(os.path.join(test_path,filename),target_size=(256,256), color_mode = color_mode)\n",
    "        img=img_to_array(img)/255\n",
    "        X_test.append(img.copy())\n",
    "    X_test_label = np.array(X_test)\n",
    "    return X_test_label\n",
    "\n",
    "test_img = test('dataset/images_satelite')\n",
    "\n",
    "print(test_img.shape)\n",
    "\n",
    "test_label = test('dataset/labels_satelite', color_mode = \"grayscale\")\n",
    "print(test_label.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 1s 52ms/sample - loss: 1.5818 - accuracy: 0.6035\n",
      "Test score: 1.581809163093567\n",
      "Test accuracy: 0.6035216\n"
     ]
    }
   ],
   "source": [
    "model=satellite_unet(pretrained='vanilla.hdf5')\n",
    "\n",
    "score, acc = model.evaluate(test_img, test_label)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 images belonging to 1 classes.\n",
      "Found 27 images belonging to 1 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2000 steps\n",
      "Epoch 1/5\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.2735 - mean_io_u_1: 0.3362\n",
      "Epoch 00001: saving model to sat_unet_IoU.hdf5\n",
      "2000/2000 [==============================] - 274s 137ms/step - loss: 0.2735 - mean_io_u_1: 0.3362\n",
      "Epoch 2/5\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.1675 - mean_io_u_1: 0.3360\n",
      "Epoch 00002: saving model to sat_unet_IoU.hdf5\n",
      "2000/2000 [==============================] - 270s 135ms/step - loss: 0.1675 - mean_io_u_1: 0.3361\n",
      "Epoch 3/5\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.1270 - mean_io_u_1: 0.3361\n",
      "Epoch 00003: saving model to sat_unet_IoU.hdf5\n",
      "2000/2000 [==============================] - 270s 135ms/step - loss: 0.1269 - mean_io_u_1: 0.3362\n",
      "Epoch 4/5\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.1024 - mean_io_u_1: 0.3364\n",
      "Epoch 00004: saving model to sat_unet_IoU.hdf5\n",
      "2000/2000 [==============================] - 270s 135ms/step - loss: 0.1023 - mean_io_u_1: 0.3364\n",
      "Epoch 5/5\n",
      "1999/2000 [============================>.] - ETA: 0s - loss: 0.0857 - mean_io_u_1: 0.3371\n",
      "Epoch 00005: saving model to sat_unet_IoU.hdf5\n",
      "2000/2000 [==============================] - 270s 135ms/step - loss: 0.0857 - mean_io_u_1: 0.3371\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f12302f3a50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gen_args = dict(rotation_range=0.2,\n",
    "                    width_shift_range=0.05,\n",
    "                    height_shift_range=0.05,\n",
    "                    shear_range=0.05,\n",
    "                    zoom_range=0.05,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True,\n",
    "                    fill_mode='nearest')\n",
    "myGene = trainGenerator(3,'dataset','images','labels',data_gen_args,save_to_dir = None)\n",
    "model = satellite_unet()\n",
    "model_checkpoint = ModelCheckpoint('sat_unet_IoU.hdf5', monitor='acc',verbose=1, save_best_only=False)\n",
    "model.fit_generator(myGene,steps_per_epoch=2000,epochs=5,callbacks=[model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 2s 166ms/sample - loss: 0.5189 - accuracy: 0.7684\n",
      "Test score: 0.518913209438324\n",
      "Test accuracy: 0.7684392\n"
     ]
    }
   ],
   "source": [
    "model=satellite_unet(pretrained='sat_unet_.hdf5')\n",
    "\n",
    "score, acc = model.evaluate(test_img, test_label)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(\n",
    "    inputs,\n",
    "    use_batch_norm=True,\n",
    "    dropout=0.3,\n",
    "    dropout_type=\"spatial\",\n",
    "    filters=16,\n",
    "    kernel_size=(3, 3),\n",
    "    activation=\"relu\",\n",
    "    kernel_initializer=\"he_normal\",\n",
    "    padding=\"same\",\n",
    "):\n",
    "\n",
    "    if dropout_type == \"spatial\":\n",
    "        DO = SpatialDropout2D\n",
    "    elif dropout_type == \"standard\":\n",
    "        DO = Dropout\n",
    "    else:\n",
    "        raise ValueError(f\"dropout_type must be one of ['spatial', 'standard'], got {dropout_type}\")\n",
    "\n",
    "    c = Conv2D(filters,kernel_size,activation=activation,kernel_initializer=kernel_initializer,\n",
    "        padding=padding,use_bias=not use_batch_norm,)(inputs)\n",
    "    \n",
    "    if use_batch_norm:\n",
    "        c = BatchNormalization()(c)\n",
    "    if dropout > 0.0:\n",
    "        c = DO(dropout)(c)\n",
    "    c = Conv2D(filters, kernel_size, activation=activation, kernel_initializer=kernel_initializer,\n",
    "        padding=padding, use_bias=not use_batch_norm,)(c)\n",
    "    if use_batch_norm:\n",
    "        c = BatchNormalization()(c)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_unet(\n",
    "    input_shape=(256,256,3),\n",
    "    num_classes=1,\n",
    "    dropout=0.5, \n",
    "    filters=64,\n",
    "    num_layers=4,\n",
    "    output_activation='sigmoid'): # 'sigmoid' or 'softmax'\n",
    "\n",
    "    # Build U-Net model\n",
    "    inputs = Input(input_shape)\n",
    "    x = inputs   \n",
    "    print('1', x.shape)\n",
    "\n",
    "    down_layers = []\n",
    "    for l in range(num_layers):\n",
    "        x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='valid')\n",
    "        print('l', x.shape)\n",
    "        down_layers.append(x)\n",
    "        x = MaxPooling2D((2, 2), strides=2) (x)\n",
    "        print('l', x.shape)\n",
    "        filters = filters*2 # double the number of filters with each layer\n",
    "\n",
    "    x = Dropout(dropout)(x)\n",
    "    x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='valid')\n",
    "    print('11', x.shape)\n",
    "\n",
    "    for conv in reversed(down_layers):\n",
    "        \n",
    "        filters //= 2 # decreasing number of filters with each layer \n",
    "        x = Conv2DTranspose(filters, (2, 2), strides=(2, 2), padding='valid') (x)\n",
    "        print('conv1', x.shape)\n",
    "        \n",
    "        ch, cw = get_crop_shape(int_shape(conv), int_shape(x))\n",
    "        conv = Cropping2D(cropping=(ch, cw))(conv)\n",
    "        print('conv2', conv.shape)\n",
    "\n",
    "        x = concatenate([x, conv])\n",
    "        print('concatenate',x.shape)\n",
    "        x = conv2d_block(inputs=x, filters=filters, use_batch_norm=False, dropout=0.0, padding='valid')\n",
    "        print('final', x.shape)\n",
    "    \n",
    "    outputs = Conv2D(num_classes, (1, 1), activation=output_activation) (x)    \n",
    "    print('output', outputs.shape)\n",
    "    \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_crop_shape(target, refer):\n",
    "    # width, the 3rd dimension\n",
    "    cw = target[2] - refer[2]\n",
    "    assert (cw >= 0)\n",
    "    if cw % 2 != 0:\n",
    "        cw1, cw2 = int(cw/2), int(cw/2) + 1\n",
    "    else:\n",
    "        cw1, cw2 = int(cw/2), int(cw/2)\n",
    "    # height, the 2nd dimension\n",
    "    ch = target[1] - refer[1]\n",
    "    assert (ch >= 0)\n",
    "    if ch % 2 != 0:\n",
    "        ch1, ch2 = int(ch/2), int(ch/2) + 1\n",
    "    else:\n",
    "        ch1, ch2 = int(ch/2), int(ch/2)\n",
    "\n",
    "    return (ch1, ch2), (cw1, cw2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "metadata": {
   "deletable": true,
   "editable": true,
   "trusted": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
